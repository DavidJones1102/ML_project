{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bmMWnxVU5CY",
        "outputId": "af61d0e1-9f3d-4f8a-95b1-d8ae71ca1799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'interPLM' already exists and is not an empty directory.\n",
            "/content/interPLM\n",
            "Obtaining file:///content/interPLM\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Installing collected packages: interplm\n",
            "  Attempting uninstall: interplm\n",
            "    Found existing installation: interplm 1.0.0\n",
            "    Uninstalling interplm-1.0.0:\n",
            "      Successfully uninstalled interplm-1.0.0\n",
            "  Running setup.py develop for interplm\n",
            "Successfully installed interplm-1.0.0\n",
            "Requirement already satisfied: biopython in /usr/local/lib/python3.12/dist-packages (1.86)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from biopython) (2.0.2)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ElanaPearl/interPLM.git\n",
        "%cd interPLM\n",
        "!pip install -e .\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWbqY1-N2V6W"
      },
      "source": [
        "## Pobieranie danych"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAXyGUVUDPGD"
      },
      "source": [
        "## Zbiór Danych: CATH Protein Domains\n",
        "\n",
        "\n",
        "W bazie CATH istnieje ścisła hierarchia.\n",
        "\n",
        "| Poziom | Oznaczenie | Nazwa | Liczba Klas* |\n",
        "| --- | --- | --- | --- |\n",
        "| **C** | `1` | **Class** | 5 |\n",
        "| **A** | `1.10` | **Architecture** | 26 |\n",
        "| **T** | `1.10.8` | **Topology** | 520  |\n",
        "| **H** | `1.10.8.10` | **Homology** | 671 |\n",
        "\n",
        "**Liczby klas są przybliżone i zależą od wersji bazy CATH.*\n",
        "\n",
        "Będziemy trenować model na poziomie C.A.T, ponieważ niższe poziomy są zbyt ogólne, a poziom C.A.T.H jest zbyt rozdrobniony.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTV5m71cxjDf",
        "outputId": "ec0b3e97-27c7-4478-c29b-4416006c9a8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pobieranie etykiet...\n",
            "Pobieranie sekwencji...\n",
            "total 154M\n",
            "-rw-r--r-- 1 szymon szymon  43M Jan 20 19:40 cath-domain-list.txt\n",
            "-rw-r--r-- 1 szymon szymon 112M Jan 20 19:44 cath-domain-seqs.fa\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "mkdir -p cath_data\n",
        "cd cath_data\n",
        "\n",
        "echo \"Pobieranie etykiet...\"\n",
        "wget -q -nc ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/cath-classification-data/cath-domain-list.txt\n",
        "\n",
        "echo \"Pobieranie sekwencji...\"\n",
        "wget -q -nc ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/sequence-data/cath-domain-seqs.fa\n",
        "ls -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab98WWb1283J",
        "outputId": "39d2dbb3-bd9c-4c55-ac9d-b9672a56ec05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#---------------------------------------------------------------------\n",
            "# FILE NAME:    CathDomainList.v4.4.0\n",
            "# FILE DATE:    16.12.2024\n",
            "#\n",
            "# CATH VERSION: v4.4.0\n",
            "# VERSION DATE: 16.12.2024\n",
            "#\n",
            "# FILE FORMAT:  Cath List File (CLF) Format 2.0\n",
            "#\n",
            "# FILE DESCRIPTION:\n",
            "# Contains all classified protein domains in CATH\n",
            "# for class 1 (mainly alpha), class 2 (mainly beta),\n",
            "# class 3 (alpha and beta) and class 4 (few secondary structures).\n",
            "#\n",
            "# See 'README.file_formats' for file format information\n",
            "#---------------------------------------------------------------------\n",
            "1oaiA00     1    10     8    10     1     1     1     1     1    59 1.000\n",
            "1go5A00     1    10     8    10     1     1     1     1     2    69 999.000\n",
            "3frhA01     1    10     8    10     2     1     1     1     1    58 1.200\n",
            "3friA01     1    10     8    10     2     1     1     1     2    54 1.800\n"
          ]
        }
      ],
      "source": [
        "!head -20 cath_data/cath-domain-list.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "LABEL_FILE = 'cath_data/cath-domain-list.txt'\n",
        "SEQ_FILE = 'cath_data/cath-domain-seqs.fa'\n",
        "column_names = [\n",
        "    'domain_id', 'class_C', 'arch_A', 'top_T', 'hom_H',\n",
        "    's35', 's60', 's95', 's100', 's100_count', 'domain_len', 'resolution'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_labels = pd.read_csv(\n",
        "    LABEL_FILE,\n",
        "    sep=r'\\s+',\n",
        "    comment='#',\n",
        "    header=None,\n",
        "    names=column_names,\n",
        "    usecols=['domain_id', 'class_C', 'arch_A', 'top_T', 'hom_H']\n",
        ")\n",
        "\n",
        "df_labels['target_label'] = (\n",
        "    df_labels['class_C'].astype(str) + \".\" +\n",
        "    df_labels['arch_A'].astype(str) + \".\" +\n",
        "    df_labels['top_T'].astype(str)\n",
        ")\n",
        "\n",
        "print(df_labels[['class_C', 'arch_A', 'top_T', 'hom_H']].nunique())\n",
        "print(\"Rozkład klas (C):\")\n",
        "print(df_labels['class_C'].value_counts())\n",
        "\n",
        "top_families = df_labels['hom_H'].value_counts()\n",
        "print(\"\\nTop 5 najliczniejszych rodzin:\")\n",
        "print(top_families.head(5))\n",
        "\n",
        "print(\"\\n5 najmniej licznych rodzin:\")\n",
        "print(top_families.tail(5))\n",
        "\n",
        "max_share = (top_families.iloc[0] / len(df_labels)) * 100\n",
        "print(f\"\\nNajwiększa rodzina stanowi {max_share:.2f}% całego zbioru.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJaFW0-2-nCw",
        "outputId": "5b812fc0-a04c-4634-8304-9213c7497e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wczytywanie sekwencji z cath_data/cath-domain-seqs.fa...\n",
            "Łączenie sekwencji z etykietami\n",
            "--------------------------------------------------\n",
            "GOTOWY ZBIÓR DANYCH: 601328 próbek.\n",
            "--------------------------------------------------\n",
            "  domain_id target_label                                           sequence\n",
            "0   1oaiA00       1.10.8  PTLSPEQQEMLQAFSTQSGMNLEWSQKCLQDNNWDYTRSAQAFTHL...\n",
            "1   1go5A00       1.10.8  PAPTPSSSPVPTLSPEQQEMLQAFSTQSGMNLEWSQKCLQDNNWDY...\n",
            "2   3frhA01       1.10.8  YPMNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEA...\n",
            "3   3friA01       1.10.8  YPMNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEA...\n",
            "4   3b89A01       1.10.8  SLNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEAA...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "\n",
        "df_labels = pd.read_csv(\n",
        "    LABEL_FILE,\n",
        "    sep=r'\\s+',\n",
        "    comment='#',\n",
        "    header=None,\n",
        "    names=column_names,\n",
        "    usecols=['domain_id', 'class_C', 'arch_A', 'top_T', 'hom_H']\n",
        ")\n",
        "print(df_labels[['class_C', 'arch_A', 'top_T', 'hom_H']].nunique())\n",
        "df_labels['target_label'] = (\n",
        "    df_labels['class_C'].astype(str) + \".\" +\n",
        "    df_labels['arch_A'].astype(str) + \".\" +\n",
        "    df_labels['top_T'].astype(str)\n",
        ")\n",
        "df_labels.drop(columns=['class_C', 'arch_A', 'top_T', 'hom_H'], inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "print(f\"Wczytywanie sekwencji z {SEQ_FILE}\")\n",
        "seq_data = []\n",
        "\n",
        "with open(SEQ_FILE, \"r\") as handle:\n",
        "    for record in SeqIO.parse(handle, \"fasta\"):\n",
        "        original_id = record.id\n",
        "        # original_id = cath|4_4_0|3avrA01/886-901_1246-1312_1380-1395\n",
        "        try:\n",
        "            part_with_id = original_id.split('|')[2]\n",
        "            # part_with_id = 3avrA01/886-901_1246-1312_1380-1395\n",
        "\n",
        "            clean_id = part_with_id.split('/')[0]\n",
        "            # clean_id = 3avrA01\n",
        "            seq_data.append({\n",
        "                'domain_id': clean_id,\n",
        "                'sequence': str(record.seq)\n",
        "            })\n",
        "        except IndexError:\n",
        "            print(f\"Pominięto nietypowy nagłówek: {original_id}\")\n",
        "            continue\n",
        "\n",
        "df_seqs = pd.DataFrame(seq_data)\n",
        "\n",
        "\n",
        "print(\"Łączenie sekwencji z etykietami\")\n",
        "full_dataset = pd.merge(df_labels, df_seqs, on='domain_id', how='inner')\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"GOTOWY ZBIÓR DANYCH: {len(full_dataset)} próbek.\")\n",
        "print(\"-\" * 50)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "print(full_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXr9JS5MVqqG",
        "outputId": "d683ca35-d317-47ce-c429-2276cf1aba87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Używam urządzenia: cuda\n",
            "Ładowanie modelu ESM-2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t6_8M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ładowanie SAE dla warstwy 6...\n",
            "Loading configs from /root/.cache/huggingface/hub/models--Elana--InterPLM-esm2-8m/snapshots/81d2429cd9dae7175f1dcd8b4c649a20cdc06c8c/layer_6/config.yaml\n",
            "Loaded data type: <class 'interplm.train.configs.TrainingRunConfig'>\n",
            "Data keys: Not a dict\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ReLUSAE(\n",
              "  (encoder): Linear(in_features=320, out_features=10240, bias=True)\n",
              "  (decoder): Linear(in_features=10240, out_features=320, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from interplm.sae.inference import load_sae_from_hf\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Używam urządzenia: {DEVICE}\")\n",
        "\n",
        "MODEL_NAME = \"esm2-8m\"\n",
        "HF_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "LAYER_ID = 6\n",
        "\n",
        "print(\"Ładowanie modelu ESM-2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
        "base_model = AutoModel.from_pretrained(HF_MODEL_NAME).to(DEVICE)\n",
        "base_model.eval()\n",
        "\n",
        "print(f\"Ładowanie SAE dla warstwy {LAYER_ID}...\")\n",
        "sae = load_sae_from_hf(plm_model=MODEL_NAME, plm_layer=LAYER_ID)\n",
        "sae = sae.to(DEVICE)\n",
        "sae.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6-2WYy1Wpdw",
        "outputId": "11276fac-a5c1-4e05-ac64-0942936d86f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Przetwarzanie 10000 próbek...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 313/313 [00:56<00:00,  5.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Wymiary macierzy cech X: (10000, 10240)\n",
            "Liczba etykiet y: 10000\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- OGRANICZENIE DANYCH DO TESTÓW ---\n",
        "SAMPLE_SIZE = 100_000\n",
        "\n",
        "if SAMPLE_SIZE:\n",
        "    df_subset = full_dataset.sample(n=min(SAMPLE_SIZE, len(full_dataset)), random_state=42).copy()\n",
        "else:\n",
        "    df_subset = full_dataset.copy()\n",
        "\n",
        "print(f\"Przetwarzanie {len(df_subset)} próbek...\")\n",
        "\n",
        "def extract_sae_features(sequences, batch_size=32):\n",
        "    all_features = []\n",
        "\n",
        "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
        "        batch_seqs = sequences[i:i+batch_size]\n",
        "\n",
        "        inputs = tokenizer(\n",
        "            batch_seqs,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            max_length=512\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Przepuszczamy sekwencje przez ESM-2\n",
        "            outputs = base_model(**inputs, output_hidden_states=True)\n",
        "\n",
        "            # Pobieramy hidden state z wybranej warstwy\n",
        "            dense_acts = outputs.hidden_states[LAYER_ID]\n",
        "\n",
        "            # Przepuszczamy pobraną warstwe InterPLM\n",
        "            sae_acts = sae.encode(dense_acts)\n",
        "\n",
        "            # SAE zwraca osobne cechy dla każdego aminokwasu\n",
        "            # uśredniamy, aby uzyskać jeden wektor opisujący całe białko dla klasyfikatora.\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1).float()\n",
        "            sum_features = torch.sum(sae_acts * mask, dim=1)\n",
        "            count_tokens = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "            mean_features = sum_features / count_tokens\n",
        "\n",
        "            all_features.append(mean_features.cpu().numpy())\n",
        "\n",
        "    return np.vstack(all_features)\n",
        "\n",
        "X = extract_sae_features(df_subset['sequence'].tolist())\n",
        "y = df_subset['target_label'].values\n",
        "\n",
        "print(f\"\\nWymiary macierzy cech X: {X.shape}\")\n",
        "print(f\"Liczba etykiet y: {len(y)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9bU6AKtqeOF"
      },
      "source": [
        "## Trening na CPU.\n",
        "Działa ale dla dużej ilości danych zapycha RAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69_1aDXfWrLM"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Filtrowanie rzadkich klas\n",
        "unique_classes, counts = np.unique(y, return_counts=True)\n",
        "valid_classes = unique_classes[counts > 1]\n",
        "\n",
        "mask_valid = np.isin(y, valid_classes)\n",
        "X_filtered = X[mask_valid]\n",
        "y_filtered = y[mask_valid]\n",
        "\n",
        "print(f\"Oryginalna liczba próbek: {len(y)}\")\n",
        "print(f\"Po usunięciu pojedynczych klas: {len(y_filtered)}\")\n",
        "print(f\"Liczba unikalnych klas do przewidzenia: {len(valid_classes)}\")\n",
        "\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_filtered)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered,\n",
        "    y_encoded,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_encoded\n",
        ")\n",
        "\n",
        "clf = LogisticRegression(\n",
        "    max_iter=3000,\n",
        "    solver='lbfgs',\n",
        "    C=1.0,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nTrening modelu\")\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Ewaluacja\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"-\" * 40)\n",
        "print(f\"Accuracy: {acc:.2%}\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# Wyświetlamy raport tylko dla klas obecnych w zbiorze testowym\n",
        "unique_test_labels = np.unique(y_test)\n",
        "target_names = le.inverse_transform(unique_test_labels)\n",
        "\n",
        "print(\"\\nRaport klasyfikacji:\")\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    labels=unique_test_labels,\n",
        "    target_names=target_names,\n",
        "    zero_division=0\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "se9QMkd_qlZd"
      },
      "source": [
        "## Trening na GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWOQujOBd_CJ",
        "outputId": "a30fb452-77db-4512-a245-94d86b35ef24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trenowanie na: cuda\n",
            "Rozpoczynam trening na GPU...\n",
            "Epoch [100/1000], Loss: 1.4051\n",
            "Epoch [200/1000], Loss: 0.7287\n",
            "Epoch [300/1000], Loss: 0.4689\n",
            "Epoch [400/1000], Loss: 0.3354\n",
            "Epoch [500/1000], Loss: 0.2556\n",
            "Epoch [600/1000], Loss: 0.2030\n",
            "Epoch [700/1000], Loss: 0.1659\n",
            "Epoch [800/1000], Loss: 0.1384\n",
            "Epoch [900/1000], Loss: 0.1174\n",
            "Epoch [1000/1000], Loss: 0.1009\n",
            "Ewaluacja...\n",
            "----------------------------------------\n",
            "Accuracy (GPU): 81.16%\n",
            "----------------------------------------\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     1.10.10       0.62      0.69      0.65        26\n",
            "    1.10.100       1.00      1.00      1.00         1\n",
            "   1.10.1030       0.00      0.00      0.00         1\n",
            "   1.10.1040       0.40      0.67      0.50         3\n",
            "   1.10.1050       1.00      1.00      1.00         1\n",
            "   1.10.1060       1.00      1.00      1.00         1\n",
            "   1.10.1070       1.00      1.00      1.00         1\n",
            "    1.10.110       0.00      0.00      0.00         1\n",
            "   1.10.1130       0.00      0.00      0.00         1\n",
            "   1.10.1140       0.67      1.00      0.80         2\n",
            "   1.10.1170       1.00      1.00      1.00         1\n",
            "     1.10.12       0.00      0.00      0.00         2\n",
            "   1.10.1200       1.00      0.67      0.80         3\n",
            "   1.10.1220       1.00      1.00      1.00         1\n",
            "   1.10.1280       0.33      1.00      0.50         1\n",
            "   1.10.1300       1.00      1.00      1.00         2\n",
            "    1.10.132       0.00      0.00      0.00         1\n",
            "   1.10.1410       1.00      1.00      1.00         1\n",
            "    1.10.150       0.60      0.82      0.69        11\n",
            "   1.10.1650       1.00      1.00      1.00         1\n",
            "   1.10.1660       1.00      1.00      1.00         1\n",
            "    1.10.167       0.00      0.00      0.00         1\n",
            "   1.10.1670       0.00      0.00      0.00         1\n",
            "   1.10.1760       1.00      1.00      1.00         1\n",
            "   1.10.1840       1.00      1.00      1.00         5\n",
            "     1.10.20       0.77      1.00      0.87        10\n",
            "   1.10.2020       1.00      1.00      1.00         1\n",
            "    1.10.220       0.33      0.33      0.33         3\n",
            "    1.10.230       0.00      0.00      0.00         1\n",
            "    1.10.238       1.00      0.67      0.80         9\n",
            "    1.10.240       1.00      1.00      1.00         1\n",
            "    1.10.245       1.00      1.00      1.00         1\n",
            "    1.10.246       1.00      1.00      1.00         6\n",
            "    1.10.260       1.00      0.67      0.80         3\n",
            "    1.10.274       0.00      0.00      0.00         2\n",
            "    1.10.275       0.00      0.00      0.00         3\n",
            "    1.10.286       0.00      0.00      0.00         1\n",
            "    1.10.287       0.51      0.67      0.58        27\n",
            "     1.10.30       1.00      1.00      1.00         1\n",
            "   1.10.3210       1.00      1.00      1.00         1\n",
            "    1.10.340       0.00      0.00      0.00         1\n",
            "   1.10.3460       0.00      0.00      0.00         1\n",
            "    1.10.357       0.80      1.00      0.89         4\n",
            "   1.10.3730       0.00      0.00      0.00         1\n",
            "    1.10.375       1.00      1.00      1.00         3\n",
            "   1.10.3860       1.00      1.00      1.00         1\n",
            "    1.10.390       0.00      0.00      0.00         1\n",
            "     1.10.40       0.00      0.00      0.00         1\n",
            "    1.10.400       0.00      0.00      0.00         1\n",
            "    1.10.405       0.00      0.00      0.00         1\n",
            "    1.10.418       0.00      0.00      0.00         1\n",
            "    1.10.420       1.00      1.00      1.00         2\n",
            "    1.10.437       1.00      0.50      0.67         2\n",
            "    1.10.439       0.00      0.00      0.00         1\n",
            "    1.10.442       1.00      1.00      1.00         1\n",
            "     1.10.45       0.00      0.00      0.00         1\n",
            "    1.10.455       0.00      0.00      0.00         1\n",
            "    1.10.472       0.80      0.80      0.80         5\n",
            "    1.10.490       0.73      0.79      0.76        14\n",
            "    1.10.510       1.00      1.00      1.00        31\n",
            "    1.10.520       1.00      0.50      0.67         2\n",
            "    1.10.530       0.75      0.86      0.80         7\n",
            "    1.10.533       1.00      1.00      1.00         1\n",
            "    1.10.540       1.00      1.00      1.00         2\n",
            "    1.10.560       0.67      1.00      0.80         2\n",
            "    1.10.565       0.89      1.00      0.94         8\n",
            "    1.10.580       0.00      0.00      0.00         1\n",
            "     1.10.60       1.00      1.00      1.00         1\n",
            "    1.10.600       0.67      1.00      0.80         4\n",
            "    1.10.601       1.00      1.00      1.00         1\n",
            "    1.10.620       1.00      0.67      0.80         3\n",
            "    1.10.630       1.00      1.00      1.00         6\n",
            "    1.10.645       1.00      1.00      1.00         2\n",
            "    1.10.750       1.00      1.00      1.00         1\n",
            "    1.10.760       1.00      1.00      1.00         4\n",
            "      1.10.8       0.29      0.40      0.33        10\n",
            "    1.10.840       0.00      0.00      0.00         1\n",
            "   1.20.1050       0.60      0.75      0.67         8\n",
            "   1.20.1060       1.00      1.00      1.00         1\n",
            "   1.20.1070       0.60      1.00      0.75         3\n",
            "   1.20.1090       1.00      1.00      1.00         1\n",
            "    1.20.120       0.39      0.58      0.47        12\n",
            "   1.20.1250       0.00      0.00      0.00         2\n",
            "   1.20.1260       1.00      1.00      1.00        13\n",
            "   1.20.1270       1.00      0.50      0.67         2\n",
            "   1.20.1280       0.00      0.00      0.00         2\n",
            "   1.20.1300       0.00      0.00      0.00         1\n",
            "   1.20.1310       1.00      1.00      1.00         1\n",
            "   1.20.1370       1.00      1.00      1.00         1\n",
            "    1.20.140       0.50      0.50      0.50         4\n",
            "   1.20.1420       0.00      0.00      0.00         1\n",
            "   1.20.1440       0.50      0.67      0.57         3\n",
            "    1.20.150       1.00      1.00      1.00         1\n",
            "   1.20.1640       1.00      1.00      1.00         1\n",
            "    1.20.190       1.00      1.00      1.00         2\n",
            "     1.20.20       0.80      1.00      0.89         4\n",
            "    1.20.200       1.00      0.50      0.67         2\n",
            "    1.20.272       0.00      0.00      0.00         1\n",
            "      1.20.5       1.00      0.21      0.35        14\n",
            "     1.20.58       0.57      0.33      0.42        12\n",
            "     1.20.80       0.00      0.00      0.00         1\n",
            "     1.20.82       1.00      1.00      1.00         1\n",
            "    1.20.840       1.00      1.00      1.00         1\n",
            "     1.20.85       1.00      1.00      1.00         3\n",
            "    1.20.890       0.00      0.00      0.00         1\n",
            "     1.20.90       1.00      1.00      1.00         2\n",
            "    1.20.920       0.67      1.00      0.80         8\n",
            "    1.20.930       0.00      0.00      0.00         1\n",
            "    1.20.960       1.00      1.00      1.00         1\n",
            "    1.20.970       0.00      0.00      0.00         1\n",
            "    1.20.990       0.50      1.00      0.67         1\n",
            "     1.25.10       1.00      0.75      0.86         4\n",
            "     1.25.40       0.73      0.85      0.79        13\n",
            "     1.25.50       1.00      1.00      1.00         1\n",
            "     1.50.10       1.00      0.60      0.75         5\n",
            "     2.10.10       1.00      1.00      1.00         2\n",
            "    2.10.150       0.00      0.00      0.00         1\n",
            "     2.10.25       1.00      1.00      1.00         4\n",
            "     2.10.50       0.00      0.00      0.00         1\n",
            "     2.10.60       1.00      1.00      1.00         1\n",
            "     2.10.70       1.00      1.00      1.00         3\n",
            "     2.10.77       1.00      1.00      1.00         4\n",
            "     2.10.90       1.00      0.50      0.67         2\n",
            "    2.100.10       1.00      0.50      0.67         2\n",
            "    2.102.10       1.00      1.00      1.00         1\n",
            "    2.115.10       1.00      1.00      1.00         2\n",
            "    2.120.10       1.00      0.75      0.86         4\n",
            "    2.130.10       0.92      1.00      0.96        11\n",
            "    2.140.10       1.00      1.00      1.00         2\n",
            "    2.160.10       1.00      1.00      1.00         4\n",
            "    2.160.20       1.00      1.00      1.00         2\n",
            "   2.170.120       1.00      1.00      1.00         2\n",
            "   2.170.150       0.00      0.00      0.00         2\n",
            "   2.170.270       0.00      0.00      0.00         1\n",
            "    2.170.30       1.00      1.00      1.00         1\n",
            "    2.170.40       1.00      1.00      1.00         1\n",
            "     2.20.25       0.67      0.40      0.50         5\n",
            "     2.20.28       1.00      1.00      1.00         2\n",
            "     2.20.70       1.00      1.00      1.00         1\n",
            "    2.30.110       0.00      0.00      0.00         1\n",
            "    2.30.130       0.00      0.00      0.00         1\n",
            "    2.30.140       1.00      1.00      1.00         1\n",
            "    2.30.170       0.00      0.00      0.00         1\n",
            "     2.30.26       1.00      1.00      1.00         1\n",
            "     2.30.29       0.75      0.75      0.75         4\n",
            "     2.30.30       0.71      0.77      0.74        22\n",
            "     2.30.31       1.00      1.00      1.00         1\n",
            "     2.30.33       0.00      0.00      0.00         1\n",
            "     2.30.39       0.00      0.00      0.00         1\n",
            "     2.30.40       1.00      1.00      1.00         2\n",
            "     2.30.42       1.00      1.00      1.00         4\n",
            "     2.40.10       0.94      0.97      0.96        35\n",
            "    2.40.100       0.00      0.00      0.00         1\n",
            "    2.40.110       1.00      0.50      0.67         2\n",
            "    2.40.128       0.79      0.85      0.81        13\n",
            "    2.40.150       0.50      0.50      0.50         2\n",
            "    2.40.155       1.00      1.00      1.00         5\n",
            "    2.40.160       1.00      0.67      0.80         3\n",
            "    2.40.180       1.00      1.00      1.00         2\n",
            "    2.40.240       0.00      0.00      0.00         1\n",
            "    2.40.260       1.00      1.00      1.00         1\n",
            "    2.40.270       1.00      1.00      1.00         1\n",
            "     2.40.30       0.56      0.56      0.56         9\n",
            "    2.40.320       0.00      0.00      0.00         1\n",
            "     2.40.33       1.00      1.00      1.00         1\n",
            "     2.40.37       1.00      1.00      1.00         2\n",
            "     2.40.40       0.00      0.00      0.00         2\n",
            "     2.40.50       0.53      0.67      0.59        24\n",
            "    2.40.510       1.00      1.00      1.00         1\n",
            "     2.40.70       1.00      1.00      1.00        18\n",
            "    2.60.120       0.80      0.92      0.86        49\n",
            "    2.60.130       0.00      0.00      0.00         1\n",
            "    2.60.175       1.00      1.00      1.00         2\n",
            "    2.60.200       0.00      0.00      0.00         1\n",
            "    2.60.210       1.00      1.00      1.00         1\n",
            "    2.60.260       0.00      0.00      0.00         1\n",
            "     2.60.34       1.00      1.00      1.00         1\n",
            "     2.60.40       0.89      0.95      0.92       215\n",
            "     2.60.60       0.00      0.00      0.00         1\n",
            "     2.60.90       0.00      0.00      0.00         1\n",
            "    2.70.160       1.00      1.00      1.00         2\n",
            "    2.70.170       1.00      1.00      1.00         9\n",
            "     2.70.20       0.00      0.00      0.00         1\n",
            "     2.70.40       0.00      0.00      0.00         2\n",
            "     2.70.50       1.00      0.50      0.67         2\n",
            "     2.70.70       0.00      0.00      0.00         1\n",
            "     2.70.98       1.00      0.50      0.67         2\n",
            "     2.80.10       1.00      0.83      0.91         6\n",
            "     3.10.10       1.00      1.00      1.00         2\n",
            "    3.10.100       1.00      1.00      1.00         3\n",
            "    3.10.105       0.00      0.00      0.00         1\n",
            "    3.10.110       1.00      1.00      1.00         1\n",
            "    3.10.129       1.00      0.80      0.89         5\n",
            "    3.10.130       0.50      0.50      0.50         2\n",
            "    3.10.150       0.67      1.00      0.80         2\n",
            "    3.10.170       1.00      1.00      1.00         1\n",
            "    3.10.180       1.00      1.00      1.00         2\n",
            "     3.10.20       0.71      0.59      0.65        17\n",
            "    3.10.200       1.00      0.50      0.67         4\n",
            "    3.10.270       1.00      1.00      1.00         1\n",
            "     3.10.28       1.00      1.00      1.00         2\n",
            "    3.10.290       1.00      1.00      1.00         2\n",
            "    3.10.310       0.00      0.00      0.00         1\n",
            "    3.10.320       1.00      1.00      1.00         2\n",
            "    3.10.330       0.00      0.00      0.00         1\n",
            "     3.10.40       0.00      0.00      0.00         1\n",
            "    3.10.420       1.00      1.00      1.00         1\n",
            "    3.10.430       1.00      1.00      1.00         1\n",
            "    3.10.450       0.88      0.64      0.74        11\n",
            "     3.10.50       0.67      1.00      0.80         2\n",
            "    3.10.580       1.00      1.00      1.00         2\n",
            "    3.100.10       0.00      0.00      0.00         3\n",
            "     3.15.10       0.00      0.00      0.00         1\n",
            "     3.20.10       0.00      0.00      0.00         1\n",
            "    3.20.110       1.00      1.00      1.00         1\n",
            "     3.20.16       0.00      0.00      0.00         1\n",
            "     3.20.20       0.98      0.98      0.98        65\n",
            "     3.20.70       0.00      0.00      0.00         1\n",
            "   3.30.1010       1.00      1.00      1.00         1\n",
            "   3.30.1120       0.00      0.00      0.00         2\n",
            "   3.30.1130       1.00      1.00      1.00         2\n",
            "   3.30.1140       1.00      1.00      1.00         1\n",
            "   3.30.1320       1.00      1.00      1.00         1\n",
            "   3.30.1330       0.71      0.56      0.62         9\n",
            "   3.30.1340       1.00      1.00      1.00         1\n",
            "   3.30.1360       0.56      0.71      0.62         7\n",
            "   3.30.1370       1.00      0.50      0.67         2\n",
            "   3.30.1380       1.00      1.00      1.00         1\n",
            "   3.30.1390       1.00      1.00      1.00         1\n",
            "   3.30.1430       1.00      1.00      1.00         1\n",
            "   3.30.1440       1.00      1.00      1.00         2\n",
            "   3.30.1460       0.00      0.00      0.00         1\n",
            "   3.30.1490       0.56      0.71      0.62         7\n",
            "    3.30.160       0.50      0.33      0.40         6\n",
            "   3.30.1600       1.00      1.00      1.00         2\n",
            "   3.30.1660       0.00      0.00      0.00         1\n",
            "   3.30.1700       1.00      1.00      1.00         1\n",
            "    3.30.190       0.00      0.00      0.00         1\n",
            "    3.30.200       1.00      0.97      0.98        29\n",
            "   3.30.2010       1.00      1.00      1.00         1\n",
            "   3.30.2090       1.00      1.00      1.00         2\n",
            "    3.30.210       1.00      1.00      1.00         3\n",
            "    3.30.230       0.86      0.86      0.86         7\n",
            "   3.30.2310       0.00      0.00      0.00         1\n",
            "    3.30.260       0.00      0.00      0.00         1\n",
            "    3.30.280       1.00      1.00      1.00         1\n",
            "     3.30.30       0.00      0.00      0.00         1\n",
            "    3.30.300       1.00      0.75      0.86         4\n",
            "    3.30.310       0.00      0.00      0.00         2\n",
            "    3.30.360       0.62      0.71      0.67         7\n",
            "    3.30.365       0.75      1.00      0.86         3\n",
            "    3.30.380       0.00      0.00      0.00         1\n",
            "    3.30.390       0.33      0.43      0.38         7\n",
            "     3.30.40       1.00      0.75      0.86         4\n",
            "    3.30.410       0.00      0.00      0.00         1\n",
            "    3.30.420       0.91      0.91      0.91        23\n",
            "    3.30.428       0.50      1.00      0.67         1\n",
            "    3.30.429       1.00      0.33      0.50         3\n",
            "     3.30.43       0.00      0.00      0.00         1\n",
            "    3.30.450       1.00      0.89      0.94         9\n",
            "     3.30.46       1.00      1.00      1.00         1\n",
            "    3.30.460       1.00      1.00      1.00         5\n",
            "    3.30.465       1.00      0.50      0.67         2\n",
            "    3.30.470       0.67      0.40      0.50         5\n",
            "    3.30.479       0.00      0.00      0.00         1\n",
            "    3.30.497       0.00      0.00      0.00         1\n",
            "     3.30.50       0.00      0.00      0.00         1\n",
            "    3.30.500       0.90      1.00      0.95         9\n",
            "    3.30.505       1.00      0.67      0.80         3\n",
            "    3.30.530       1.00      1.00      1.00         3\n",
            "    3.30.540       1.00      1.00      1.00         1\n",
            "    3.30.559       1.00      0.50      0.67         2\n",
            "    3.30.565       1.00      1.00      1.00         2\n",
            "    3.30.572       1.00      0.50      0.67         2\n",
            "    3.30.590       1.00      0.50      0.67         2\n",
            "     3.30.60       1.00      1.00      1.00         1\n",
            "     3.30.63       0.00      0.00      0.00         1\n",
            "     3.30.67       1.00      1.00      1.00         1\n",
            "     3.30.70       0.51      0.78      0.62        55\n",
            "    3.30.710       0.50      0.50      0.50         2\n",
            "    3.30.720       1.00      1.00      1.00         2\n",
            "    3.30.750       1.00      1.00      1.00         1\n",
            "    3.30.830       0.83      1.00      0.91         5\n",
            "    3.30.860       1.00      1.00      1.00         1\n",
            "    3.30.870       0.00      0.00      0.00         1\n",
            "      3.30.9       0.50      1.00      0.67         1\n",
            "    3.30.930       0.25      0.25      0.25         4\n",
            "    3.30.950       1.00      1.00      1.00         1\n",
            "   3.40.1010       1.00      1.00      1.00         1\n",
            "   3.40.1030       1.00      1.00      1.00         1\n",
            "   3.40.1050       1.00      1.00      1.00         1\n",
            "    3.40.109       1.00      0.50      0.67         2\n",
            "   3.40.1110       0.00      0.00      0.00         1\n",
            "   3.40.1160       1.00      1.00      1.00         2\n",
            "   3.40.1170       1.00      1.00      1.00         2\n",
            "   3.40.1180       1.00      1.00      1.00         1\n",
            "   3.40.1190       1.00      0.50      0.67         4\n",
            "    3.40.120       0.00      0.00      0.00         1\n",
            "   3.40.1280       1.00      0.33      0.50         3\n",
            "   3.40.1350       1.00      1.00      1.00         1\n",
            "   3.40.1360       1.00      1.00      1.00         1\n",
            "   3.40.1370       0.00      0.00      0.00         1\n",
            "   3.40.1380       1.00      1.00      1.00         2\n",
            "   3.40.1390       0.00      0.00      0.00         1\n",
            "    3.40.140       0.00      0.00      0.00         1\n",
            "   3.40.1400       0.00      0.00      0.00         1\n",
            "   3.40.1510       1.00      1.00      1.00         1\n",
            "   3.40.1620       0.00      0.00      0.00         1\n",
            "   3.40.1650       1.00      1.00      1.00         1\n",
            "    3.40.190       0.79      0.90      0.84        21\n",
            "     3.40.20       0.50      0.50      0.50         2\n",
            "    3.40.220       1.00      0.75      0.86         4\n",
            "    3.40.225       1.00      1.00      1.00         1\n",
            "    3.40.228       1.00      1.00      1.00         1\n",
            "    3.40.250       1.00      1.00      1.00         1\n",
            "     3.40.30       0.94      0.88      0.91        17\n",
            "    3.40.309       0.67      0.80      0.73         5\n",
            "    3.40.350       0.00      0.00      0.00         1\n",
            "    3.40.390       1.00      0.67      0.80         3\n",
            "    3.40.430       1.00      0.67      0.80         3\n",
            "    3.40.449       0.00      0.00      0.00         1\n",
            "    3.40.462       0.00      0.00      0.00         1\n",
            "     3.40.47       1.00      0.88      0.93         8\n",
            "    3.40.470       0.50      1.00      0.67         1\n",
            "      3.40.5       1.00      1.00      1.00         1\n",
            "     3.40.50       0.79      0.97      0.87       240\n",
            "    3.40.605       1.00      1.00      1.00         6\n",
            "    3.40.630       1.00      0.83      0.91        12\n",
            "    3.40.640       1.00      1.00      1.00        11\n",
            "    3.40.710       1.00      1.00      1.00         9\n",
            "    3.40.718       1.00      0.67      0.80         3\n",
            "    3.40.720       1.00      1.00      1.00         1\n",
            "     3.40.80       1.00      1.00      1.00         1\n",
            "    3.40.800       1.00      1.00      1.00         3\n",
            "    3.40.830       1.00      1.00      1.00         1\n",
            "    3.40.850       1.00      1.00      1.00         1\n",
            "     3.40.91       1.00      1.00      1.00         1\n",
            "    3.40.980       1.00      1.00      1.00         1\n",
            "     3.50.30       1.00      1.00      1.00         2\n",
            "      3.50.4       1.00      1.00      1.00         1\n",
            "     3.50.50       1.00      1.00      1.00        12\n",
            "      3.50.7       1.00      0.50      0.67         2\n",
            "     3.50.80       0.00      0.00      0.00         1\n",
            "     3.55.40       0.50      1.00      0.67         1\n",
            "    3.60.110       1.00      1.00      1.00         1\n",
            "    3.60.130       1.00      1.00      1.00         2\n",
            "     3.60.15       1.00      1.00      1.00         5\n",
            "     3.60.20       0.85      0.90      0.88        39\n",
            "     3.60.21       1.00      1.00      1.00         2\n",
            "     3.60.40       1.00      1.00      1.00         1\n",
            "     3.60.70       1.00      1.00      1.00         1\n",
            "     3.65.10       1.00      0.50      0.67         2\n",
            "     3.70.10       0.00      0.00      0.00         1\n",
            "     3.75.10       0.00      0.00      0.00         1\n",
            "     3.80.10       1.00      1.00      1.00         1\n",
            "     3.90.10       0.50      1.00      0.67         1\n",
            "   3.90.1030       1.00      1.00      1.00         1\n",
            "   3.90.1070       1.00      1.00      1.00         1\n",
            "    3.90.110       1.00      1.00      1.00         2\n",
            "   3.90.1110       1.00      1.00      1.00         1\n",
            "   3.90.1150       0.43      0.50      0.46        12\n",
            "   3.90.1170       1.00      0.67      0.80         3\n",
            "   3.90.1180       0.50      1.00      0.67         1\n",
            "   3.90.1200       1.00      1.00      1.00         1\n",
            "   3.90.1230       1.00      1.00      1.00         5\n",
            "   3.90.1300       1.00      1.00      1.00         1\n",
            "   3.90.1310       0.00      0.00      0.00         1\n",
            "    3.90.180       1.00      1.00      1.00         3\n",
            "    3.90.190       1.00      0.80      0.89         5\n",
            "     3.90.20       1.00      1.00      1.00         4\n",
            "    3.90.209       0.83      1.00      0.91         5\n",
            "    3.90.210       0.00      0.00      0.00         1\n",
            "    3.90.215       1.00      1.00      1.00         1\n",
            "    3.90.226       1.00      0.67      0.80         9\n",
            "    3.90.228       1.00      1.00      1.00         3\n",
            "    3.90.230       1.00      1.00      1.00         1\n",
            "     3.90.25       0.00      0.00      0.00         1\n",
            "    3.90.330       1.00      1.00      1.00         1\n",
            "    3.90.340       0.83      1.00      0.91         5\n",
            "    3.90.380       0.00      0.00      0.00         1\n",
            "    3.90.400       0.00      0.00      0.00         1\n",
            "    3.90.440       1.00      1.00      1.00         5\n",
            "     3.90.45       1.00      1.00      1.00         1\n",
            "    3.90.470       1.00      1.00      1.00         2\n",
            "    3.90.550       1.00      1.00      1.00         6\n",
            "     3.90.56       1.00      1.00      1.00         1\n",
            "    3.90.640       1.00      1.00      1.00         4\n",
            "    3.90.650       0.00      0.00      0.00         1\n",
            "    3.90.660       0.00      0.00      0.00         1\n",
            "     3.90.70       0.33      0.33      0.33         3\n",
            "    3.90.700       1.00      1.00      1.00         1\n",
            "    3.90.730       0.00      0.00      0.00         1\n",
            "     3.90.79       1.00      1.00      1.00         4\n",
            "     3.90.80       0.00      0.00      0.00         1\n",
            "    3.90.870       0.00      0.00      0.00         1\n",
            "    3.90.930       0.50      0.67      0.57         3\n",
            "    3.90.940       0.00      0.00      0.00         1\n",
            "   4.10.1240       1.00      1.00      1.00         1\n",
            "   4.10.1270       1.00      1.00      1.00         1\n",
            "    4.10.140       1.00      1.00      1.00         1\n",
            "    4.10.410       1.00      1.00      1.00         2\n",
            "    4.10.470       0.00      0.00      0.00         1\n",
            "    4.10.480       1.00      1.00      1.00         1\n",
            "    4.10.520       1.00      1.00      1.00         1\n",
            "    4.10.530       1.00      1.00      1.00         2\n",
            "    4.10.540       1.00      1.00      1.00         1\n",
            "    4.10.640       1.00      1.00      1.00         1\n",
            "     4.10.80       0.00      0.00      0.00         1\n",
            "    4.10.830       1.00      1.00      1.00         1\n",
            "     4.10.91       1.00      1.00      1.00         1\n",
            "    4.10.910       1.00      1.00      1.00         2\n",
            "    4.10.950       1.00      1.00      1.00         2\n",
            "     6.10.10       0.00      0.00      0.00         1\n",
            "    6.10.140       0.00      0.00      0.00         4\n",
            "     6.10.20       0.00      0.00      0.00         1\n",
            "    6.10.250       0.00      0.00      0.00         5\n",
            "    6.10.280       1.00      1.00      1.00         1\n",
            "     6.20.50       1.00      1.00      1.00         1\n",
            "\n",
            "   micro avg       0.81      0.81      0.81      1953\n",
            "   macro avg       0.67      0.66      0.66      1953\n",
            "weighted avg       0.79      0.81      0.79      1953\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "unique_classes, counts = np.unique(y, return_counts=True)\n",
        "valid_classes = unique_classes[counts > 1]\n",
        "mask_valid = np.isin(y, valid_classes)\n",
        "X_filtered = X[mask_valid]\n",
        "y_filtered = y[mask_valid]\n",
        "\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y_filtered)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_filtered, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Trenowanie na: {DEVICE}\")\n",
        "\n",
        "X_train_t = torch.tensor(X_train, dtype=torch.float32).to(DEVICE)\n",
        "y_train_t = torch.tensor(y_train, dtype=torch.long).to(DEVICE)\n",
        "X_test_t = torch.tensor(X_test, dtype=torch.float32).to(DEVICE)\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "output_dim = len(valid_classes)\n",
        "\n",
        "class LogisticRegressionPyTorch(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super(LogisticRegressionPyTorch, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "model = LogisticRegressionPyTorch(input_dim, output_dim).to(DEVICE)\n",
        "\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "EPOCHS = 1000\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "\n",
        "    outputs = model(X_train_t)\n",
        "    loss = criterion(outputs, y_train_t)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 100 == 0 or epoch == 0:\n",
        "        print(f'Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "print(\"Ewaluacja\")\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test_t)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    y_pred = predicted.cpu().numpy()\n",
        "\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "p = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "r = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy:       {acc:.2%}\")\n",
        "print(f\"F1:       {f1:.2%}\")\n",
        "print(f\"Precision: {p:.2%}\")\n",
        "print(f\"Recall:    {r:.2%}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "unique_test_labels = np.unique(y_test)\n",
        "target_names = le.inverse_transform(unique_test_labels)\n",
        "\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    labels=unique_test_labels,\n",
        "    target_names=target_names,\n",
        "    zero_division=0\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import xgboost as xgb\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model = xgb.XGBClassifier(\n",
        "    objective='multi:softmax',\n",
        "    num_class=len(valid_classes),\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    device=DEVICE,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Starting training\")\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Evaluation\")\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "p = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "r = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "\n",
        "print(\"-\" * 30)\n",
        "print(f\"Accuracy:       {acc:.2%}\")\n",
        "print(f\"F1:       {f1:.2%}\")\n",
        "print(f\"Precision: {p:.2%}\")\n",
        "print(f\"Recall:    {r:.2%}\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "unique_test_labels = np.unique(y_test)\n",
        "target_names = le.inverse_transform(unique_test_labels)\n",
        "\n",
        "print(classification_report(\n",
        "    y_test,\n",
        "    y_pred,\n",
        "    labels=unique_test_labels,\n",
        "    target_names=target_names,\n",
        "    zero_division=0\n",
        "))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
