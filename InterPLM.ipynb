{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Klasyfikator klasy białka na podstawie cech z InterPLM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Qp8cHOEGfW1"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ElanaPearl/interPLM.git\n",
        "%cd interPLM\n",
        "!pip install -e .\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Co chcemy osiągnąć?\n",
        "<img src=\"https://raw.githubusercontent.com/DavidJones1102/ML_project/refs/heads/main/pipeline.svg\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hWbqY1-N2V6W"
      },
      "source": [
        "## Pobieranie danych\n",
        "### Zbiór Danych: CATH Protein Domains\n",
        "\n",
        "\n",
        "W bazie CATH istnieje ścisła hierarchia.\n",
        "\n",
        "| Poziom | Oznaczenie | Nazwa | Liczba Klas* |\n",
        "| --- | --- | --- | --- |\n",
        "| **C** | `1` | **Class** | ~4 |\n",
        "| **A** | `1.10` | **Architecture** | ~40 |\n",
        "| **T** | `1.10.8` | **Topology** | **~1,272**  |\n",
        "| **H** | `1.10.8.10` | **Homology** | >6,000 |\n",
        "\n",
        "**Liczby klas są przybliżone i zależą od wersji bazy CATH.*\n",
        "\n",
        "Będziemy trenować model na poziomie C.A.T, ponieważ niższe poziomy są zbyt ogólne, a poziom C.A.T.H jest zbyt rozdrobniony.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTV5m71cxjDf",
        "outputId": "acacb216-77a2-46d3-b56b-aef6d2bcaffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Pobieranie etykiet...\n",
            "Pobieranie sekwencji...\n",
            "Pliki w katalogu:\n",
            "total 154M\n",
            "-rw-r--r-- 1 root root  43M Jan 20 10:01 cath-domain-list.txt\n",
            "-rw-r--r-- 1 root root 112M Jan 20 10:01 cath-domain-seqs.fa\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "mkdir -p cath_data\n",
        "cd cath_data\n",
        "\n",
        "echo \"Pobieranie etykiet...\"\n",
        "wget -q -nc ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/cath-classification-data/cath-domain-list.txt\n",
        "\n",
        "echo \"Pobieranie sekwencji...\"\n",
        "wget -q -nc ftp://orengoftp.biochem.ucl.ac.uk/cath/releases/latest-release/sequence-data/cath-domain-seqs.fa\n",
        "\n",
        "ls -lh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ab98WWb1283J",
        "outputId": "b735d645-4a95-4dde-fa0c-88c67fea22a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#---------------------------------------------------------------------\n",
            "# FILE NAME:    CathDomainList.v4.4.0\n",
            "# FILE DATE:    16.12.2024\n",
            "#\n",
            "# CATH VERSION: v4.4.0\n",
            "# VERSION DATE: 16.12.2024\n",
            "#\n",
            "# FILE FORMAT:  Cath List File (CLF) Format 2.0\n",
            "#\n",
            "# FILE DESCRIPTION:\n",
            "# Contains all classified protein domains in CATH\n",
            "# for class 1 (mainly alpha), class 2 (mainly beta),\n",
            "# class 3 (alpha and beta) and class 4 (few secondary structures).\n",
            "#\n",
            "# See 'README.file_formats' for file format information\n",
            "#---------------------------------------------------------------------\n",
            "1oaiA00     1    10     8    10     1     1     1     1     1    59 1.000\n",
            "1go5A00     1    10     8    10     1     1     1     1     2    69 999.000\n",
            "3frhA01     1    10     8    10     2     1     1     1     1    58 1.200\n",
            "3friA01     1    10     8    10     2     1     1     1     2    54 1.800\n"
          ]
        }
      ],
      "source": [
        "!head -20 cath_data/cath-domain-list.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJaFW0-2-nCw",
        "outputId": "e5fd7b7b-5058-4bec-8af3-62b6a5b3afd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wczytywanie sekwencji z cath_data/cath-domain-seqs.fa...\n",
            "Łączenie sekwencji z etykietami\n",
            "--------------------------------------------------\n",
            "GOTOWY ZBIÓR DANYCH: 601328 próbek.\n",
            "--------------------------------------------------\n",
            "  domain_id target_label                                           sequence\n",
            "0   1oaiA00       1.10.8  PTLSPEQQEMLQAFSTQSGMNLEWSQKCLQDNNWDYTRSAQAFTHL...\n",
            "1   1go5A00       1.10.8  PAPTPSSSPVPTLSPEQQEMLQAFSTQSGMNLEWSQKCLQDNNWDY...\n",
            "2   3frhA01       1.10.8  YPMNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEA...\n",
            "3   3friA01       1.10.8  YPMNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEA...\n",
            "4   3b89A01       1.10.8  SLNINDALTSILASKKYRALCPDTVRRILTEEWGRHKSPKQTVEAA...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from Bio import SeqIO\n",
        "\n",
        "LABEL_FILE = 'cath_data/cath-domain-list.txt'\n",
        "SEQ_FILE = 'cath_data/cath-domain-seqs.fa'\n",
        "\n",
        "column_names = [\n",
        "    'domain_id', 'class_C', 'arch_A', 'top_T', 'hom_H',\n",
        "    's35', 's60', 's95', 's100', 's100_count', 'domain_len', 'resolution'\n",
        "]\n",
        "\n",
        "df_labels = pd.read_csv(\n",
        "    LABEL_FILE,\n",
        "    sep=r'\\s+',\n",
        "    comment='#',\n",
        "    header=None,\n",
        "    names=column_names,\n",
        "    usecols=['domain_id', 'class_C', 'arch_A', 'top_T', 'hom_H']\n",
        ")\n",
        "\n",
        "df_labels['target_label'] = (\n",
        "    df_labels['class_C'].astype(str) + \".\" +\n",
        "    df_labels['arch_A'].astype(str) + \".\" +\n",
        "    df_labels['top_T'].astype(str)\n",
        ")\n",
        "df_labels.drop(columns=['class_C', 'arch_A', 'top_T', 'hom_H'], inplace=True, errors='ignore')\n",
        "\n",
        "\n",
        "print(f\"Wczytywanie sekwencji z {SEQ_FILE}...\")\n",
        "seq_data = []\n",
        "\n",
        "with open(SEQ_FILE, \"r\") as handle:\n",
        "    for record in SeqIO.parse(handle, \"fasta\"):\n",
        "        original_id = record.id\n",
        "        # original_id = cath|4_4_0|3avrA01/886-901_1246-1312_1380-1395\n",
        "        try:\n",
        "            part_with_id = original_id.split('|')[2]\n",
        "            # part_with_id = 3avrA01/886-901_1246-1312_1380-1395\n",
        "\n",
        "            clean_id = part_with_id.split('/')[0]\n",
        "            # clean_id = 3avrA01\n",
        "            seq_data.append({\n",
        "                'domain_id': clean_id,\n",
        "                'sequence': str(record.seq)\n",
        "            })\n",
        "        except IndexError:\n",
        "            print(f\"Pominięto nietypowy nagłówek: {original_id}\")\n",
        "            continue\n",
        "\n",
        "df_seqs = pd.DataFrame(seq_data)\n",
        "\n",
        "\n",
        "print(\"Łączenie sekwencji z etykietami\")\n",
        "full_dataset = pd.merge(df_labels, df_seqs, on='domain_id', how='inner')\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"GOTOWY ZBIÓR DANYCH: {len(full_dataset)} próbek.\")\n",
        "print(\"-\" * 50)\n",
        "pd.set_option('display.max_colwidth', 50)\n",
        "print(full_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from interplm.sae.inference import load_sae_from_hf\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Używam urządzenia: {DEVICE}\")\n",
        "\n",
        "MODEL_NAME = \"esm2-8m\"\n",
        "HF_MODEL_NAME = \"facebook/esm2_t6_8M_UR50D\"\n",
        "LAYER_ID = 6 \n",
        "\n",
        "print(\"Ładowanie modelu ESM-2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(HF_MODEL_NAME)\n",
        "base_model = AutoModel.from_pretrained(HF_MODEL_NAME).to(DEVICE)\n",
        "base_model.eval()\n",
        "\n",
        "print(f\"Ładowanie SAE dla warstwy {LAYER_ID}...\")\n",
        "sae = load_sae_from_hf(plm_model=MODEL_NAME, plm_layer=LAYER_ID)\n",
        "sae = sae.to(DEVICE)\n",
        "sae.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# --- OGRANICZENIE DANYCH DO TESTÓW ---\n",
        "SAMPLE_SIZE = 2000 \n",
        "\n",
        "if SAMPLE_SIZE:\n",
        "    df_subset = full_dataset.sample(n=min(SAMPLE_SIZE, len(full_dataset)), random_state=42).copy()\n",
        "else:\n",
        "    df_subset = full_dataset.copy()\n",
        "\n",
        "print(f\"Przetwarzanie {len(df_subset)} próbek...\")\n",
        "\n",
        "def extract_sae_features(sequences, batch_size=32):\n",
        "    all_features = []\n",
        "    \n",
        "    for i in tqdm(range(0, len(sequences), batch_size)):\n",
        "        batch_seqs = sequences[i:i+batch_size]\n",
        "        \n",
        "        inputs = tokenizer(\n",
        "            batch_seqs, \n",
        "            return_tensors=\"pt\", \n",
        "            padding=True, \n",
        "            truncation=True, \n",
        "            max_length=512\n",
        "        ).to(DEVICE)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            # Przepuszczamy sekwencje przez ESM-2\n",
        "            outputs = base_model(**inputs, output_hidden_states=True)\n",
        "            \n",
        "            # Pobieramy hidden state z wybranej warstwy\n",
        "            dense_acts = outputs.hidden_states[LAYER_ID] \n",
        "            \n",
        "            # Przepuszczamy pobraną warstwe InterPLM\n",
        "            sae_acts = sae.encode(dense_acts)\n",
        "            \n",
        "            # SAE zwraca osobne cechy dla każdego aminokwasu\n",
        "            # uśredniamy, aby uzyskać jeden wektor opisujący całe białko dla klasyfikatora.\n",
        "            mask = inputs['attention_mask'].unsqueeze(-1).float()\n",
        "            sum_features = torch.sum(sae_acts * mask, dim=1)\n",
        "            count_tokens = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
        "            mean_features = sum_features / count_tokens\n",
        "            \n",
        "            all_features.append(mean_features.cpu().numpy())\n",
        "            \n",
        "    return np.vstack(all_features)\n",
        "\n",
        "X = extract_sae_features(df_subset['sequence'].tolist())\n",
        "y = df_subset['target_label'].values\n",
        "\n",
        "print(f\"\\nWymiary macierzy cech X: {X.shape}\")\n",
        "print(f\"Liczba etykiet y: {len(y)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
